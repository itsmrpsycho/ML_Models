{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1143, 13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dat1 = pd.read_csv(\"./bin/WineQT.csv\")\n",
    "print(dat1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = dat1.drop(columns=['quality','Id']).to_numpy()\n",
    "Y = dat1['quality'].to_numpy()\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=4)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=345)\n",
    "\n",
    "# Handle Missing or Inconsistent Data (if necessary)\n",
    "# Use SimpleImputer to fill missing values with the mean or other strategies\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Normalize and Standardize Data\n",
    "# Use StandardScaler to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Model Building from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class MLPClassifier2:\n",
    "    def __init__(self, n_features, n_classes, learning_rate, n_hiddenLayers, n_nodesPerLayer, activateFunc, epochs, optimizer=\"Batch\",printOutput=True) -> None:\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_hiddenLayers = n_hiddenLayers\n",
    "        self.n_nodesPerLayer = n_nodesPerLayer\n",
    "        self.activateFunc = activateFunc\n",
    "        self.printOutput = printOutput\n",
    "        self.epochs = epochs\n",
    "        self.uniques = None\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.layeredWeights = [np.random.rand(n_features, n_nodesPerLayer)]\n",
    "        self.layeredBiases = [np.random.rand(n_nodesPerLayer)]\n",
    "\n",
    "        for i in range(1, n_hiddenLayers):\n",
    "            self.layeredWeights.append(np.random.rand(n_nodesPerLayer, n_nodesPerLayer))\n",
    "            self.layeredBiases.append(np.random.rand(n_nodesPerLayer))\n",
    "        \n",
    "        self.layeredWeights.append(np.random.rand(n_nodesPerLayer, n_classes))\n",
    "        self.layeredBiases.append(np.random.rand(n_classes))\n",
    "    \n",
    "    def printMetrics(self,preds,true):\n",
    "        print(f\"Accuracy = \\t {accuracy_score(preds,true)}\")\n",
    "        print(f\"precision micro = \\t {precision_score(preds,true,average='micro',zero_division=1)}\",end=\"\\t\")\n",
    "        print(f\"precision macro = \\t {precision_score(preds,true,average='macro',zero_division=1)}\")\n",
    "        print(f\"recall micro = \\t\\t {recall_score(preds,true,average='micro',zero_division=1)}\",end=\" \\t\")\n",
    "        print(f\"recall macro = \\t\\t {recall_score(preds,true,average='macro',zero_division=1)}\")\n",
    "        print(f\"f1_score micro = \\t {f1_score(preds,true,average='micro',zero_division=1)}\",end=\"\\t\")\n",
    "        print(f\"f1_score macro = \\t {f1_score(preds,true,average='macro',zero_division=1)}\\n\")\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, oneHotTrueProbs, pred_probs):\n",
    "        epsilon = 1e-15\n",
    "        n = oneHotTrueProbs.shape[0]\n",
    "        pred_probs = np.maximum(epsilon, pred_probs)\n",
    "        loss = -np.sum(oneHotTrueProbs * np.log(pred_probs))\n",
    "        \n",
    "        return loss/n\n",
    "    \n",
    "    # def oneHotLabels2(self, Y):\n",
    "    #     oneHotTrueProbs = pd.get_dummies(Y).to_numpy()\n",
    "    #     return oneHotTrueProbs\n",
    "    \n",
    "    def oneHotLabels(self, Y):\n",
    "        out = []\n",
    "        categories = self.uniques\n",
    "        for i in Y:\n",
    "            num_categories = len(categories)\n",
    "            encoding = np.eye(num_categories)\n",
    "            index = np.where(categories==i)\n",
    "            out.append(encoding[index].squeeze())\n",
    "        return np.array(out)\n",
    "\n",
    "    def activationFunc(self,X):\n",
    "        if self.activateFunc == \"ReLU\":\n",
    "            return np.maximum(0, X)\n",
    "        elif self.activateFunc == \"tanh\":\n",
    "            return np.tanh(X)\n",
    "        elif self.activateFunc == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def derivActivation(self,X):\n",
    "        if self.activateFunc == \"ReLU\":\n",
    "            return np.maximum(0, X)\n",
    "        elif self.activateFunc == \"tanh\":\n",
    "            return 1 - np.tanh(X) ** 2\n",
    "        elif self.activateFunc == \"sigmoid\":\n",
    "            return (1 / (1 + np.exp(-X))) * (1 - (1 / (1 + np.exp(-X))))\n",
    "    \n",
    "    def backward(self, x, y, outputs, activations):\n",
    "        m = x.shape[0]\n",
    "        yOneHot = self.oneHotLabels(y)\n",
    "        dZs = [activations[-1] - yOneHot]\n",
    "        dWs = [(1/m) * activations[-2].T @ dZs[-1]]\n",
    "        dBs = [(1/m) * np.sum(dZs[-1], axis = 0)]\n",
    "\n",
    "        for i in range(self.n_hiddenLayers):\n",
    "            dZs.append(dZs[-1] @ self.layeredWeights[-1-i].T * self.derivActivation(outputs[-2-i]))\n",
    "            dWs.append((1/m) * activations[-3-i].T @ dZs[-1])\n",
    "            dBs.append((1/m) * np.sum(dZs[-1], axis=0))\n",
    "\n",
    "\n",
    "        dWs.reverse()\n",
    "        dBs.reverse()\n",
    "        return dWs, dBs\n",
    "\n",
    "    def forward(self,X):\n",
    "        curMat = X\n",
    "        out = []\n",
    "        activ = [X]\n",
    "        for i in range(self.n_hiddenLayers):\n",
    "            curMat = curMat @ self.layeredWeights[i] + self.layeredBiases[i]\n",
    "            out.append(curMat)\n",
    "            curMat = self.activationFunc(curMat)\n",
    "            activ.append(curMat)\n",
    "        \n",
    "        curMat = curMat @ self.layeredWeights[-1] + self.layeredBiases[-1]\n",
    "        out.append(curMat)\n",
    "        curMat = self.softmax(curMat)\n",
    "        activ.append(curMat)\n",
    "        \n",
    "        return out, activ\n",
    "    \n",
    "    def update(self, dWs,dBs):\n",
    "        for i in range(self.n_hiddenLayers + 1):\n",
    "            self.layeredWeights[i] -= (self.learning_rate * dWs[i])\n",
    "            self.layeredBiases[i] -= (self.learning_rate * dBs[i])\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X):\n",
    "        outputs, activations = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def evaluate_wandb(self,X,Y):\n",
    "        probabilities = self.predict(X)\n",
    "        oneHotY = self.oneHotLabels(Y)\n",
    "\n",
    "        loss = self.cross_entropy_loss(oneHotY, probabilities)\n",
    "        preds = self.predict(X)\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        yOut = np.argmax(oneHotY, axis=1)\n",
    "        acc = accuracy_score(preds, yOut)\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val):\n",
    "        self.uniques = np.unique(Y_train)\n",
    "        N = int(len(X_train))\n",
    "        batchSize = int(N/10)\n",
    "        for epoch in range(self.epochs):\n",
    "            if (epoch%100 == 0 and self.printOutput==True):\n",
    "                preds = self.predict(X_val)\n",
    "                preds = np.argmax(preds, axis=1)\n",
    "\n",
    "                yOneHot = self.oneHotLabels(Y_val)\n",
    "                yOut = np.argmax(yOneHot, axis=1)\n",
    "                print(f\"Epoch = {epoch}\")\n",
    "                self.printMetrics(preds,yOut)\n",
    "\n",
    "            \n",
    "            if (self.optimizer == \"SGD\"):\n",
    "                for i in range(N):\n",
    "                    out, activations = self.forward(X_train[i].reshape(1,self.n_features))\n",
    "                    dWs, dBs = self.backward(X_train[i], Y_train[i].reshape(1,1), out, activations)\n",
    "                    self.update(dWs,dBs)\n",
    "            elif (self.optimizer == \"BatchGD\"):\n",
    "                out, activations = self.forward(X_train)\n",
    "                dWs, dBs = self.backward(X_train, Y_train, out, activations)\n",
    "                self.update(dWs,dBs)\n",
    "            elif (self.optimizer == \"MiniBatchGD\"):\n",
    "                batchSize = int(N/10)\n",
    "                for i in range(0, N, batchSize):\n",
    "                    out, activations = self.forward(X_train[i:min(batchSize+i, N)])\n",
    "                    dWs, dBs = self.backward(X_train[i:min(batchSize+i, N)], Y_train[i:min(batchSize+i, N)], out, activations)\n",
    "                    self.update(dWs,dBs)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n",
      "Accuracy = \t 0.029239766081871343\n",
      "precision micro = \t 0.029239766081871343\tprecision macro = \t 0.1388888888888889\n",
      "recall micro = \t\t 0.029239766081871343 \trecall macro = \t\t 0.505175983436853\n",
      "f1_score micro = \t 0.029239766081871343\tf1_score macro = \t 0.34331337325349304\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100\n",
      "Accuracy = \t 0.5906432748538012\n",
      "precision micro = \t 0.5906432748538012\tprecision macro = \t 0.24174951670809167\n",
      "recall micro = \t\t 0.5906432748538012 \trecall macro = \t\t 0.863232074438971\n",
      "f1_score micro = \t 0.5906432748538012\tf1_score macro = \t 0.2168276704419276\n",
      "\n",
      "Epoch = 200\n",
      "Accuracy = \t 0.5614035087719298\n",
      "precision micro = \t 0.5614035087719298\tprecision macro = \t 0.25702581502084404\n",
      "recall micro = \t\t 0.5614035087719298 \trecall macro = \t\t 0.820718350870227\n",
      "f1_score micro = \t 0.5614035087719298\tf1_score macro = \t 0.25930921566826365\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 300\n",
      "Accuracy = \t 0.5730994152046783\n",
      "precision micro = \t 0.5730994152046783\tprecision macro = \t 0.24112485369734749\n",
      "recall micro = \t\t 0.5730994152046783 \trecall macro = \t\t 0.8656233002027395\n",
      "f1_score micro = \t 0.5730994152046783\tf1_score macro = \t 0.22555663242230403\n",
      "\n",
      "Epoch = 400\n",
      "Accuracy = \t 0.49707602339181284\n",
      "precision micro = \t 0.49707602339181284\tprecision macro = \t 0.20895142094396443\n",
      "recall micro = \t\t 0.49707602339181284 \trecall macro = \t\t 0.8315972222222223\n",
      "f1_score micro = \t 0.49707602339181284\tf1_score macro = \t 0.19581835049491284\n",
      "\n",
      "Epoch = 500\n",
      "Accuracy = \t 0.5555555555555556\n",
      "precision micro = \t 0.5555555555555556\tprecision macro = \t 0.24919286963611734\n",
      "recall micro = \t\t 0.5555555555555556 \trecall macro = \t\t 0.7939431913116124\n",
      "f1_score micro = \t 0.5555555555555556\tf1_score macro = \t 0.24718843166547022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 600\n",
      "Accuracy = \t 0.5614035087719298\n",
      "precision micro = \t 0.5614035087719298\tprecision macro = \t 0.26747905735063976\n",
      "recall micro = \t\t 0.5614035087719298 \trecall macro = \t\t 0.7893448159405606\n",
      "f1_score micro = \t 0.5614035087719298\tf1_score macro = \t 0.2640546592079023\n",
      "\n",
      "Epoch = 700\n",
      "Accuracy = \t 0.543859649122807\n",
      "precision micro = \t 0.543859649122807\tprecision macro = \t 0.2659223313738641\n",
      "recall micro = \t\t 0.543859649122807 \trecall macro = \t\t 0.78883480649785\n",
      "f1_score micro = \t 0.543859649122807\tf1_score macro = \t 0.26412785419028123\n",
      "\n",
      "Epoch = 800\n",
      "Accuracy = \t 0.5263157894736842\n",
      "precision micro = \t 0.5263157894736842\tprecision macro = \t 0.2531874252048237\n",
      "recall micro = \t\t 0.5263157894736842 \trecall macro = \t\t 0.7918367346938776\n",
      "f1_score micro = \t 0.5263157894736842\tf1_score macro = \t 0.25419553711462756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 900\n",
      "Accuracy = \t 0.5146198830409356\n",
      "precision micro = \t 0.5146198830409356\tprecision macro = \t 0.2485961520758538\n",
      "recall micro = \t\t 0.5146198830409356 \trecall macro = \t\t 0.7987196994467203\n",
      "f1_score micro = \t 0.5146198830409356\tf1_score macro = \t 0.2530609121518212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features=X_train.shape[1]\n",
    "n_classes=len(np.unique(Y_train))\n",
    "lr = 0.01\n",
    "model = MLPClassifier2(n_features, n_classes, lr, n_hiddenLayers=2,n_nodesPerLayer=5,activateFunc=\"ReLU\", optimizer=\"SGD\", epochs=1000)\n",
    "model.train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Model Training & Hyperparameter Tuning using W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:eop22tf4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Activation Function</td><td>▁▁▅▅██▁▁▅▅██▁▁▅▅█▁▁▅▅██▁▁▅▅██▁▁▅▅█▁▁▅▅██</td></tr><tr><td>Hidden Layers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██████</td></tr><tr><td>Optimization method</td><td>▁▅▁▅▁▅▁█▁█▁█▅█▅█▅▁▅▁▅▁▅▁█▁█▁█▅█▅█▅▁▅▁▅▁▅</td></tr><tr><td>accuracy</td><td>▇▅▆▆▇▄▇██▇█▇▄▇▆▇▄▆██▆█▃▄▆▇▇▇▃▅▅▅█▃▁▇▆▄▃▃</td></tr><tr><td>epochs</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁█▂▇▂█▁▂▂▂▂▂▇▂▆▃█▅▃▂█▂▇▆▆▂▂▂▇▅▃▇▂▇▇▆▃█▇▇</td></tr><tr><td>nodesPerLayer</td><td>▁▁▁▁▁▁▅▅▅▅▅▅█████▁▁▁▁▁▁▅▅▅▅▅▅█████▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Activation Function</td><td>3</td></tr><tr><td>Hidden Layers</td><td>3</td></tr><tr><td>Optimization method</td><td>6</td></tr><tr><td>accuracy</td><td>0.4152</td></tr><tr><td>epochs</td><td>1000</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>loss</td><td>1.22907</td></tr><tr><td>nodesPerLayer</td><td>3</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-valley-4</strong> at: <a href='https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier/runs/eop22tf4' target=\"_blank\">https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier/runs/eop22tf4</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_201740-eop22tf4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:eop22tf4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/itsmrpsycho/Study/SMAI/Assignment 3/assignment-3-itsmrpsycho/wandb/run-20231023_204827-45ylh2nr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier/runs/45ylh2nr' target=\"_blank\">denim-jazz-2</a></strong> to <a href='https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier' target=\"_blank\">https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier/runs/45ylh2nr' target=\"_blank\">https://wandb.ai/its_mrpsycho/SMAI_A3_q2-MultiLayerPerceptronClassifier/runs/45ylh2nr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m optimizer \u001b[39min\u001b[39;00m optimizers:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Create and train your model with the current hyperparameters\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     model \u001b[39m=\u001b[39m MLPClassifier2(n_features, n_classes, lr, n_hiddenLayers\u001b[39m=\u001b[39mnLayers,n_nodesPerLayer\u001b[39m=\u001b[39mnNodes,activateFunc\u001b[39m=\u001b[39mactivFunc,epochs\u001b[39m=\u001b[39mnum_epochs, optimizer\u001b[39m=\u001b[39moptimizer ,printOutput\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(X_train, Y_train, X_val, Y_val)\n\u001b[1;32m     60\u001b[0m     \u001b[39m# Evaluate on the validation set\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     val_loss, val_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate_wandb(X_val, Y_val)\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mMLPClassifier2.train\u001b[0;34m(self, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, N, batchSize):\n\u001b[1;32m    164\u001b[0m     out, activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X_train[i:\u001b[39mmin\u001b[39m(batchSize\u001b[39m+\u001b[39mi, N)])\n\u001b[0;32m--> 165\u001b[0m     dWs, dBs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(X_train[i:\u001b[39mmin\u001b[39;49m(batchSize\u001b[39m+\u001b[39;49mi, N)], Y_train[i:\u001b[39mmin\u001b[39;49m(batchSize\u001b[39m+\u001b[39;49mi, N)], out, activations)\n\u001b[1;32m    166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(dWs,dBs)\n",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m, in \u001b[0;36mMLPClassifier2.backward\u001b[0;34m(self, x, y, outputs, activations)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, x, y, outputs, activations):\n\u001b[1;32m     82\u001b[0m     m \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 83\u001b[0m     yOneHot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moneHotLabels(y)\n\u001b[1;32m     84\u001b[0m     dZs \u001b[39m=\u001b[39m [activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m yOneHot]\n\u001b[1;32m     85\u001b[0m     dWs \u001b[39m=\u001b[39m [(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm) \u001b[39m*\u001b[39m activations[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m dZs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m, in \u001b[0;36mMLPClassifier2.oneHotLabels\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m Y:\n\u001b[1;32m     59\u001b[0m     num_categories \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(categories)\n\u001b[0;32m---> 60\u001b[0m     encoding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49meye(num_categories)\n\u001b[1;32m     61\u001b[0m     index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(categories\u001b[39m==\u001b[39mi)\n\u001b[1;32m     62\u001b[0m     out\u001b[39m.\u001b[39mappend(encoding[index]\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/miniconda3/envs/coding/lib/python3.8/site-packages/numpy/lib/twodim_base.py:227\u001b[0m, in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype, order, like)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     i \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mk) \u001b[39m*\u001b[39m M\n\u001b[0;32m--> 227\u001b[0m m[:M\u001b[39m-\u001b[39mk]\u001b[39m.\u001b[39mflat[i::M\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39mreturn\u001b[39;00m m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"SMAI_A3_q2-MultiLayerPerceptronClassifier\", entity=\"its_mrpsycho\")\n",
    "\n",
    "myDict = {\n",
    "    \"ReLU\":1,\n",
    "    \"tanh\":2,\n",
    "    \"sigmoid\":3,\n",
    "    \"SGD\":4,\n",
    "    \"BatchGD\":5,\n",
    "    \"MiniBatchGD\":6\n",
    "}\n",
    "\n",
    "data = [\n",
    "    [\"ReLU\", 1],\n",
    "    [\"tanh\", 2],\n",
    "    [\"sigmoid\", 3],\n",
    "    [\"SGD\", 4],\n",
    "    [\"BatchGD\", 5],\n",
    "    [\"MiniBatchGD\", 6]\n",
    "]\n",
    "\n",
    "# Define a function to log to W&B\n",
    "def log_to_wandb(loss, accuracy, learning_rate, epochs, hLayers, nodesPerLayer, optimizer, activFunc):\n",
    "    wandb.log({\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"Hidden Layers\": hLayers,\n",
    "        \"nodesPerLayer\": nodesPerLayer,\n",
    "        \"Optimization method\": optimizer,\n",
    "        \"Activation Function\": activFunc,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs\n",
    "    })\n",
    "\n",
    "# Hyperparameter Search\n",
    "learning_rates = [0.01, 0.1]\n",
    "epochs = [1000]\n",
    "hiddenLayers = [1,2,3]\n",
    "nodesPerLayer = [3,4,5]\n",
    "activationFuncs = [\"ReLU\",\"tanh\",\"sigmoid\"]\n",
    "optimizers = [\"SGD\",\"BatchGD\",\"MiniBatchGD\"]\n",
    "\n",
    "columns = [\"Category\", \"Value\"]\n",
    "table = wandb.Table(data=data, columns=columns)\n",
    "wandb.log({\"custom_table\": table})\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_epochs in epochs:\n",
    "        for nLayers in hiddenLayers:\n",
    "            for nNodes in nodesPerLayer:\n",
    "                for activFunc in activationFuncs:\n",
    "                    for optimizer in optimizers:\n",
    "                        # Create and train your model with the current hyperparameters\n",
    "                        model = MLPClassifier2(n_features, n_classes, lr, n_hiddenLayers=nLayers,n_nodesPerLayer=nNodes,activateFunc=activFunc,epochs=num_epochs, optimizer=optimizer ,printOutput=False)\n",
    "                        model.train(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "                        # Evaluate on the validation set\n",
    "                        val_loss, val_accuracy = model.evaluate_wandb(X_val, Y_val)\n",
    "\n",
    "                        # Log the results to W&B\n",
    "                        log_to_wandb(val_loss, val_accuracy, float(lr), num_epochs, nLayers, nNodes, myDict[optimizer], myDict[activFunc])\n",
    "# Analyze the results in the W&B dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       1.00      0.00      0.00         3\n",
      "           2       1.00      0.00      0.00        74\n",
      "           3       1.00      0.00      0.00        76\n",
      "           4       1.00      0.00      0.00        17\n",
      "           5       0.01      1.00      0.01         1\n",
      "\n",
      "    accuracy                           0.01       172\n",
      "   macro avg       0.83      0.17      0.00       172\n",
      "weighted avg       0.99      0.01      0.00       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "modelOptimal = MLPClassifier2(n_features, n_classes, learning_rate=0.1, n_hiddenLayers=3,n_nodesPerLayer=7,activateFunc=\"ReLU\",epochs=1500,printOutput=False)\n",
    "\n",
    "modelOptimal.train(X_train, Y_train, X_val, Y_val)\n",
    "preds = modelOptimal.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "yOut = np.argmax(modelOptimal.oneHotLabels(Y_test), axis=1)\n",
    "report = classification_report(yOut, preds,zero_division=1)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
